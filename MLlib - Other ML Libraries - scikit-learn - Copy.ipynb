{"cells":[{"cell_type":"markdown","source":["# Using scikit-learn with Spark on Databricks\n\nThis notebook demonstrates how to take advantage of Spark and Databricks to use [scikit-learn](http://scikit-learn.org/), the popular Python library for doing Machine Learning on a single compute node.\n\nEven though the algorithms in scikit-learn are not distributed, we can still take advantage of distributed computation for certain ML tasks.  This can help with the transition from single-node workflows to fully distributed workflows: One can start by porting an existing workflow to Spark, begin to distribute certain tasks, and eventually move to fully distributed training via MLlib algorithms.\n\n**Contents**\n1. Running scikit-learn on the driver\n2. Distributing scikit-learn jobs\n3. Converting between scikit-learn and MLlib models"],"metadata":{}},{"cell_type":"markdown","source":["## 1. Running scikit-learn on the driver\n\nThe simplest way to use scikit-learn with Spark and Databricks is to run scikit-learn jobs as usual.  However, this will run scikit-learn jobs on the driver, so **be careful** not to run large jobs, especially if other users are working on the same cluster as you.  Nevertheless, a reasonable way to port existing scikit-learn workflows to Spark and start benefiting from distributed computing is to: (a) copy the workflow into Databricks and (b) start parallelizing the workflow piece-by-piece.  We discuss parallelization in the next section.\n\nIn this section, we will do the following:\n* Load data into a Pandas dataframe\n* Explore the data\n* Transform features\n* Hold out a random test dataset\n* Learn an initial model\n* Evaluate the initial model"],"metadata":{}},{"cell_type":"markdown","source":["### Load data into a Pandas dataframe\n\nWe will use the R \"diamonds\" dataset from the \"ggplot2\" package.  This is a dataset hosted on Databricks.  To learn more about importing data, see [Accessing Data Notebook](../../03 Data Sources/0 Accessing Data.html).\n\nOur task will be to predict the price of a diamond from its properties."],"metadata":{}},{"cell_type":"code","source":["displayHTML(sc.wholeTextFiles(\"/databricks-datasets/Rdatasets/data-001/doc/ggplot2/diamonds.html\").take(1)[0][1])"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# Load data into a Pandas dataframe\nimport pandas\nimport cStringIO\nfrom pyspark.sql import *\nlocalData = sc.wholeTextFiles(\"/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv\").collect()[0][1]\noutput = cStringIO.StringIO(localData)\npandasData = pandas.read_csv(output)\npandasData = pandasData.iloc[:,1:] # remove line number"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["### Explore the data\n\nWe quickly demonstrate how to start exploring the data.  For a longer tutorial, see the [Visualizations Notebook](../../01 Databricks Overview/15 Visualizations/0 Visualizations Overview.html)."],"metadata":{}},{"cell_type":"code","source":["# We can view the Pandas dataframe using Pandas' native display\npandasData"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["We can make plots using Python tools like matplotlib.  For more examples, see the [Matplotlib and GGPlot Notebook](../../01 Databricks Overview/15 Visualizations/4 Matplotlib and GGPlot.html)."],"metadata":{}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\nplt.clf()\nplt.plot(pandasData['carat'], pandasData['price'], '.')\nplt.xlabel('carat')\nplt.ylabel('price')\ndisplay()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["We can also convert the Pandas dataframe into a Spark DataFrame and use DBC's display methods."],"metadata":{}},{"cell_type":"code","source":["# Create this plot by calling display on the Spark DataFrame, clicking the plot icon, selecting Plot Options, and creating a Histogram of 'carat' values.\nsparkDataframe = sqlContext.createDataFrame(pandasData)\ndisplay(sparkDataframe)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["### Transform features\n\nSome of our features are text, and we want them to be numerical so we can train a linear model.  We use the Pandas and scikit-learn APIs for these transformations."],"metadata":{}},{"cell_type":"markdown","source":["First, we convert the features to numerical values, in the correct order based on the feature meanings.  Higher indices are \"better.\"  This ordering will help us interpret model weights later on."],"metadata":{}},{"cell_type":"code","source":["pandasData['cut'] = pandasData['cut'].replace({'Fair':0, 'Good':1, 'Very Good':2, 'Premium':3, 'Ideal':4})\npandasData['color'] = pandasData['color'].replace({'J':0, 'I':1, 'H':2, 'G':3, 'F':4, 'E':5, 'D':6})\npandasData['clarity'] = pandasData['clarity'].replace({'I1':0, 'SI1':1, 'SI2':2, 'VS1':3, 'VS2':4, 'VVS1':5, 'VVS2':6, 'IF':7})\npandasData"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["Now, we normalize each feature (column) to have unit variance.  (This normalization or standardization often improves performance. See [Wikipedia](http://en.wikipedia.org/wiki/Feature_scaling#Standardization) for more info.)"],"metadata":{}},{"cell_type":"code","source":["# Split data into a labels dataframe and a features dataframe\nlabels = pandasData['price']\nfeatureNames = ['carat', 'cut', 'color', 'clarity', 'depth', 'table', 'x', 'y', 'z']\nfeatures = pandasData[featureNames]"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# Normalize features (columns) to have unit variance\nfrom sklearn.preprocessing import normalize\nfeatures = normalize(features, axis=0)\nfeatures"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["### Hold out a random test set\n\nWe hold out a random sample of the data for testing.  Note that this randomness can cause this notebook to produce different results each time it is run."],"metadata":{}},{"cell_type":"code","source":["# Hold out 30% of the data for testing.  We will use the rest for training.\nfrom sklearn.cross_validation import train_test_split\ntrainingLabels, testLabels, trainingFeatures, testFeatures = train_test_split(labels, features, test_size=0.3)\nntrain, ntest = len(trainingLabels), len(testLabels)\nprint 'Split data randomly into 2 sets: %d training and %d test instances.' % (ntrain, ntest)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["### Learn an initial model\n\nHere, we train a single model using fixed hyperparameters on the driver.  Later, we will do model tuning by training models in a distributed fashion."],"metadata":{}},{"cell_type":"code","source":["# Train a model with fixed hyperparameters, and print out the intercept and coefficients.\nfrom sklearn import linear_model\norigAlpha = 0.5 # \"alpha\" is the regularization hyperparameter\norigClf = linear_model.Ridge(alpha=origAlpha)\norigClf.fit(features, labels)\nprint 'Trained model with fixed alpha = %g' % origAlpha\nprint '  Model intercept: %g' % origClf.intercept_\nprint '  Model coefficients:'\nfor i in range(len(featureNames)):\n  print '    %g\\t%s' % (origClf.coef_[i], featureNames[i])"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["One can draw conclusions about the model coefficients and the affect of features.  However, be wary of several issues:\n* Feature meaning: Especially if you index or transform features, be careful about how those transformations can change the meaning.  E.g., reversing an index order or negating a numerical feature can \"flip\" the meaning.\n* Model assumptions: The model may not fit the data, in which case interpreting coefficients may be difficult.  E.g., if the data do not correspond to a linear model, the model may learn non-intuitive weights for some features (in its attempt to fit the data as well as possible)."],"metadata":{}},{"cell_type":"markdown","source":["### Evaluate the initial model\n\nWe will evaluate this and other models using [scikit-learn's score function](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge.score), which computes a value indicating the quality of the model's predictions on data.  A value closer to `1` is better."],"metadata":{}},{"cell_type":"code","source":["# Score the initial model.  It does not do that well.\norigScore = origClf.score(trainingFeatures, trainingLabels)\norigScore"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["## 2. Distributing scikit-learn jobs\n\nNow that we have a basic scikit-learn workflow in Databricks, we can start distributing tasks.  There are several types of tasks one might distribute, such as ETL, parameter tuning, and evaluation.  We demonstrate using Spark to distribute *parameter tuning* below."],"metadata":{}},{"cell_type":"markdown","source":["### Parameter tuning using Spark\n\n[Parameter tuning](http://en.wikipedia.org/wiki/Hyperparameter_optimization) is the task of tuning (hyper)parameters of a learning or prediction system in order to improve the results.  It is commonly done by training multiple models (each using different parameters) on one set of data and then testing those models on another held-out set of data (and maybe repeating).  By testing on a held-out set not seen during training, we can tune the parameters in a data-driven way while limiting the risk of [overfitting](http://en.wikipedia.org/wiki/Overfitting).\n\nIn this section, we will use [k-fold cross validation](http://en.wikipedia.org/wiki/Cross-validation_&#40;statistics&#41;), which works as follows:\n* Randomly split the data into k equal-sized subsets (\"folds\").\n* For ```i = 1, 2, ..., k```,\n  * Hold out fold ```i``` as a validation set.\n  * Create a training set by combining all folds except for ```i```.\n  * For each set of parameters,\n    * Train a model with that set of parameters.\n    * Test the model on the validation set to compute a validation error.\n* For each set of parameters,\n  * Compute the average validation error (averaging over the ```k``` models for this set of parameters).\n* Choose the best set of parameters, based on the average validation error.\n* Re-train on the entire dataset, using this best set of parameters.\n\nNote that for each (fold, parameter set) pair, the task of training a model can be done independently of other folds and parameter sets.  We will parallelize these tasks: scikit-learn will be used on each worker to do the training.  This parallelization is especially helpful since training is the most computationally costly part of this workflow.  If you use `k` folds of cross validation to test `P` different parameter settings, then distributing the task to train 1 model per worker can make it run close to `k*P` times faster!\n\nWe will also hold out some additional data for testing.  We will use it to demonstrate the worth of careful parameter tuning by comparing:\n* Our initial model (with poorly chosen parameters)\n* The final model (with carefully tuned parameters)"],"metadata":{}},{"cell_type":"markdown","source":["#### Split data and define tasks to distribute\n\nEach distributed task will be a (fold, parameter set) pair.  It will correspond to 1 model we train."],"metadata":{}},{"cell_type":"code","source":["# We use scikit-learn's cross_validation module, which helps split our data randomly into k equal-size parts (\"folds\").\nfrom sklearn import cross_validation\nnumFolds = 3 # You may want to use more (10 or so) in practice\nkf = cross_validation.KFold(ntrain, n_folds=numFolds)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["# \"alphas\" is a list of hyperparameter values to test\nalphas = [0.0, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n# Create a list of tasks to distribute\ntasks = []\nfor alpha in alphas:\n  for fold in range(numFolds):\n    tasks = tasks + [(alpha, fold)]"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["# Create an RDD of tasks.  We set the number of partitions equal to the number of tasks to ensure maximum parallelism.\ntasksRDD = sc.parallelize(tasks, numSlices = len(tasks))"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["#### Broadcast dataset\n\nIf we use a variable in a function (a \"closure\") run on each worker, Spark will automatically send the dataset to the workers.  This is fine for variables with small values, but for our dataset, we can send it to workers more efficiently by *broadcasting* it.  We now create a *broadcast variable* for our data, which we will use later when running tasks on workers.  For more info on broadcast variables, see the [Spark programming guide](https://spark.apache.org/docs/latest/programming-guide.html#broadcast-variables)."],"metadata":{}},{"cell_type":"code","source":["trainingFeaturesBroadcast = sc.broadcast(trainingFeatures)\ntrainingLabelsBroadcast = sc.broadcast(trainingLabels)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["#### Run cross-validation in parallel\n\nWe define a function which will run on each worker.  This function takes 1 task (1 hyperparameter alpha value + 1 fold index) and trains the corresponding model.  We then use `RDD.map` to run these tasks in parallel."],"metadata":{}},{"cell_type":"code","source":["def trainOneModel(alpha, fold):\n  \"\"\"\n  Given 1 task (1 hyperparameter alpha value + 1 fold index), train the corresponding model.\n  Return: model, score on the fold's test data, task info.\n  \"\"\"\n  # Extract indices for this fold\n  trainIndex, valIndex = [], []\n  fold_ = 0 # index into folds 'kf'\n  for trainIndex_, valIndex_ in kf:\n    if fold_ == fold:\n      trainIndex, valIndex = trainIndex_, valIndex_\n      break\n    fold_ += 1\n  # Get training data from the broadcast variables\n  localTrainingFeatures = trainingFeaturesBroadcast.value\n  localTrainingLabels = trainingLabelsBroadcast.value\n  X_train, X_val = localTrainingFeatures[trainIndex], localTrainingFeatures[valIndex]\n  Y_train, Y_val = localTrainingLabels[trainIndex], localTrainingLabels[valIndex]\n  # Train the model, and score it\n  clf = linear_model.Ridge(alpha=alpha)\n  clf.fit(X_train, Y_train)\n  score = clf.score(X_val, Y_val)\n  return clf, score, alpha, fold"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["# LEARN!  We now map our tasks RDD and apply the training function to each task.\n# After we call an action (\"count\") on the results, the actual training is executed.\ntrainedModelAndScores = tasksRDD.map(lambda alpha_fold: trainOneModel(alpha_fold[0], alpha_fold[1]))\ntrainedModelAndScores.cache()\ntrainedModelAndScores.count()"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["# Since we are done with our broadcast variables, we can clean them up.\n# (This will happen automatically, but we can make it happen earlier by explicitly unpersisting the broadcast variables.\ntrainingFeaturesBroadcast.unpersist()\ntrainingLabelsBroadcast.unpersist()"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["#### Collect results to get the best hyperparameter alpha"],"metadata":{}},{"cell_type":"code","source":["# Collect the results.\nallScores = trainedModelAndScores.map(lambda x: (x[1], x[2], x[3])).collect()\n# Average scores over folds\navgScores = dict(map(lambda alpha: (alpha, 0.0), alphas))\nfor score, alpha, fold in allScores:\n  avgScores[alpha] += score\nfor alpha in alphas:\n  avgScores[alpha] /= numFolds\navgScores"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["We now have a list of alpha values paired with the corresponding average scores (averaged over the k folds).  Let's identify the best score to discover the best value for alpha."],"metadata":{}},{"cell_type":"code","source":["# Find best score\nbestAlpha = -1\nbestScore = -1\nfor alpha in alphas:\n  if avgScores[alpha] > bestScore:\n    bestAlpha = alpha\n    bestScore = avgScores[alpha]\nprint 'Found best alpha: %g, which gives score: %g' % (bestAlpha, bestScore)"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["We can also use plotting to examine how the hyperparameter affects performance."],"metadata":{}},{"cell_type":"code","source":["# Use Databricks' display() function to plot the scores vs. alpha.  We use a namedtuple to tell Databricks names for the columns (alpha and the score).\nimport numpy\nfrom collections import namedtuple\nScore = namedtuple('Score', 'log_alpha score')\ndisplay(map(lambda alpha: Score(float(numpy.log(alpha + 0.00000001)), float(avgScores[alpha])), avgScores))"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":["For this dataset, the best alpha is generally small but not the smallest value.  (Remember that the results of this notebook can vary because of randomness in splitting the data.)\n\nThis demonstrates how parameter tuning can help *a lot*; our score can vary from 0 (terrible) to 0.9 (quite good)."],"metadata":{}},{"cell_type":"markdown","source":["#### Train a final model using the best hyperparameter\n\nWe use our chosen value of alpha to train a model on the entire training dataset.  Since this is a single training task, we execute it on the driver."],"metadata":{}},{"cell_type":"code","source":["# Use bestAlpha, and train a final model.\ntunedClf = linear_model.Ridge(alpha=bestAlpha)\ntunedClf.fit(trainingFeatures, trainingLabels)"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":["Let's compare our original model vs. the final model with tuned hyperparameters."],"metadata":{}},{"cell_type":"code","source":["origTrainingScore, origTestScore = origClf.score(trainingFeatures, trainingLabels), origClf.score(testFeatures, testLabels)\ntunedTrainingScore, tunedTestScore = tunedClf.score(trainingFeatures, trainingLabels), tunedClf.score(testFeatures, testLabels)\nprint 'Compare original model (without hyperparameter tuning) and final model (with tuning) on test data\\n'\nprint 'Model   \\tAlpha\\tTraining   \\tTest'\nprint 'Original\\t%g\\t%g\\t%g' % (origAlpha, origTrainingScore, origTestScore)\nprint 'Tuned   \\t%g\\t%g\\t%g' % (bestAlpha, tunedTrainingScore, tunedTestScore)"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":["The tuned model does better! (Note: Performance can vary because of randomness, but it should be better.)"],"metadata":{}},{"cell_type":"code","source":["print 'Tuned model with best alpha = %g' % bestAlpha\nprint '  Model intercept: %g' % tunedClf.intercept_\nprint '  Model coefficients:'\nfor i in range(len(featureNames)):\n  print '    %g\\t%s' % (tunedClf.coef_[i], featureNames[i])"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"markdown","source":["## 3. Converting between scikit-learn and MLlib models\n\nIt is often possible to convert between scikit-learn and MLlib models.  There is not built-in functionality yet, but we show how to do the conversion for linear models.  This can be useful to take advantage of each library's different sets of functionality."],"metadata":{}},{"cell_type":"code","source":["# Convert the scikit-learn model into an equivalent MLlib model\nfrom pyspark.mllib.regression import LinearRegressionModel\nmllibModel = LinearRegressionModel(tunedClf.coef_, tunedClf.intercept_)\nmllibModel"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["# Demonstrate that the models compute the same predictions\nsklearnPredictions = tunedClf.predict(testFeatures)\nmllibPredictions = numpy.array(map(lambda x: mllibModel.predict(x), testFeatures))\ndifferences = sklearnPredictions - mllibPredictions\nsumSquaredDifferences = sum(differences * differences)\nprint 'Total difference between scikit-learn and MLlib model predictions: %g' % sumSquaredDifferences"],"metadata":{},"outputs":[],"execution_count":52}],"metadata":{"name":"MLlib / Other ML Libraries / scikit-learn","notebookId":4216749089140490},"nbformat":4,"nbformat_minor":0}
